SenseDev – Comprehensive System Architecture & Planning Document (Updated)
1. Project Overview
1.1 Purpose

SenseDev is a desktop application that performs static analysis on Android (Kotlin/Java) codebases to understand sensor-driven logic:

Where sensor data enters the app

How it propagates through business logic

Where and how it reaches UI

What patterns/issues exist (lifecycle, leaks, performance, etc.)

SenseDev presents this via:

A graph of nodes and edges

A code view tied to graph nodes/edges

A details/issues panel

An optional dev-to-dev style AI Q&A panel

The core value is understanding, not code generation or mutation.

2. Core Principles

Static-analysis-first
All “facts” come from parsing and analyzing code. AI only explains.

Architecture-agnostic
Works on:

Clean MVVM + repositories

Messy god-classes, singletons, static utils

Hybrid/legacy structures

Local & non-destructive

No code changes

No builds required

No remote uploads without explicit AI use

Explainability
Every node, edge, and issue can be traced back to:

A file

A line number

A concrete AST / relationship

AI as interpreter, not oracle
AI only:

Summarizes flows

Explains connections

Answers dev questions using the graph

3. Target Users & Use Cases
3.1 Users

Android devs working with sensors:

Motion/fitness, environment, context-aware apps

Devs inheriting or onboarding into sensor-heavy codebases

Teams auditing sensor usage and lifecycle correctness

3.2 Use Cases

“How does accelerometer data reach the UI?”

“Where are all the sensors used?”

“Which components could leak memory?”

“Why does class A depend on B?”

“Explain this sensor flow in plain language.”

4. System Boundaries & Non-Goals
4.1 SenseDev does not:

Run or instrument the app

Attach to an emulator/device

Profile runtime performance

Simulate sensors

Auto-fix code or refactor architecture

Replace Android Studio

4.2 SenseDev only:

Reads source code

Builds static models

Provides visual + textual understanding

Optionally uses external LLM APIs for explanation

5. High-Level Architecture
5.1 Layers

UI Layer

Desktop UI (Compose Desktop or equivalent)

Panels: folder tree, graph, code, details, Q&A

Application Layer (Controllers)

Orchestrates UI events and engine calls

Holds UI state

Core Engine (Static Analysis)

Project indexer

Source parser (Kotlin/Java)

Symbol tables

Call graph + data flow extractor

Role classification

Graph Builder & Store

Models nodes, edges, flows

Provides query API for UI + AI layer

Issue Detection Layer

Rule-based analysis over graph + code facts

AI Integration Layer (Optional)

Connects to Gemini/OpenAI/etc.

Builds prompts from graph context

Local Storage/Cache

Per-project analysis results

User preferences

Encrypted tokens

6. Deployment & Environment

Desktop app, no server required

Targets: Windows, macOS, Linux

Packaged via JVM tooling (e.g., Compose Desktop)

Analysis is fully local; AI calls only if configured

7. Technology Stack (Planned)
Part	Tech (candidate)
UI	Kotlin + Compose Desktop
Build	Gradle
Parsing (Kotlin)	Kotlin Compiler Embeddable
Parsing (Java)	JavaParser or similar
Graph Storage	In-memory model + JSON cache
Local Store	JSON / small SQLite DB
AI Providers	Google AI API, OpenAI API
8. Modules & Responsibilities
8.1 UI Layer

Folder Tree Panel

Displays repo directory structure

Selecting a file updates:

Code view

Graph highlight (if node exists)

Q&A context

Graph Panel

Shows sensor flows and dependencies as nodes/edges

Filtering by:

Sensor type

Module/package

Node type (UI/VM/Repo/etc.)

Node interactions:

Hover → summary tooltip

Click → code + details focus

Code Panel

Read-only code view for selected file

Scrolls to relevant method on:

Node click

AI link click

Issue click

Details/Issues Panel

For selected node:

Inputs (who calls it)

Outputs (what it calls)

Sensor involvement

Issues associated

For selected flow:

Ordered node list

Summary

Q&A Panel (Chat-like)

Dev-to-dev style questions:

“Why is X connected to Y?”

“Show the path from sensor S to UI class U.”

“Which variables in this class are risky for leaks?”

History view + input box

Answers include clickable links to nodes/files/flows

Menu Bar

File: open project

View: toggle panels, filters

Connections: AI providers (Gemini, OpenAI)

Help: docs, logs

8.2 Application Layer

ProjectController

“Open project” handling

Triggers analysis

Manages current project state

GraphController

Loads graph data from engine

Applies filters

Handles node selection + syncing with UI

CodeController

Loads source text for given file

Manages code view navigation

IssueController

Surfaces issues from engine

Allows filtering and navigation

AIController

Manages provider connections

Builds context for AI queries

Handles API failures/fallbacks

9. Core Engine – Static Analysis
9.1 Current Implementation Status (Reality Check)

From your SenseMap test:

Nodes: multiple (MainActivity, ReflectionRepository, SensorRepository, SettingsRepository, PreferenceKeys)

Edges:

Only from MainActivity → each of the others

No edges between repositories, ViewModels, UI composables, etc.

Edge labels: none (edges are untyped)

This implies the current engine is doing shallow, file-level reference detection only, likely based on:

Imports

Type references

Or simple textual references

It is not yet performing:

Method-level call graph analysis

Data-flow tracking

Sensor-flow extraction

Edge typing (CALLS / USES_SENSOR_DATA / etc.)

This matches what an early MVP would do, but it is far from the intended design.

The rest of this section describes what must be added.

9.2 Target Pipeline (Intended Behavior)

Step 1 – Project Indexing

Recursively scan root directory

Collect Kotlin/Java files

Map:

filePath → package → classes

Output: file index for further parsing.

Step 2 – Parsing & Symbol Mapping

Use Kotlin compiler APIs for .kt

Use Java parser for .java

Build for each file:

AST / PSI tree

Symbol tables:

Classes

Methods

Fields

Inheritance (extends, implements)

Import resolution

Output: semantic model of the project, not just text.

Step 3 – Sensor Entry Point Detection

Identify actual sensor sources, e.g.:

SensorManager.registerListener(...)

onSensorChanged(SensorEvent event)

LocationCallback.onLocationResult(...)

Step counter/motion APIs

Light/proximity sensor callbacks

Microphone amplitude readers (non-audio)

Mark associated methods/classes as SENSOR_SOURCE nodes:

Node type = SENSOR_SOURCE

Metadata: sensor type, update frequency (if detectable), etc.

Step 4 – Call Graph Construction

This is the missing piece in your current SenseDev state.

We need a method-level call graph, not just file-level references.

For each method:

Record:

Methods it calls (directly)

Static function calls

Extension function calls

Constructor calls

This gives edges like:

SensorRepository.onSensorChanged() → MotionFilter.smooth()

ViewModel.update() → Repository.fetch()

Represented as CALLS edges in the graph.

Right now, you only have something like:

MainActivity imports SensorRepository → one unlabeled edge

You need method-level edges instead.

Step 5 – Data Flow Mapping (Within Reason)

We do not need full-blown SSA/dataflow analysis, but we need enough to:

Track sensor-derived values into:

Fields

LiveData/Flow/StateFlow variables

Parameters of downstream calls

Examples of patterns to detect:

override fun onSensorChanged(event: SensorEvent) {
    val accel = event.values[0]
    repository.updateAccel(accel)
}


→ onSensorChanged uses sensor data → passes to repository.updateAccel

Then:

fun updateAccel(accel: Float) {
    _state.value = _state.value.copy(lastAccel = accel)
}


→ Sensor-derived value written into state

Then:

val uiState by viewModel.state.collectAsState()


→ UI reads state that originates from sensor

We don’t need to track exact numeric values, just origin-tagged data paths.

Step 6 – Role Classification (Heuristic)

Assign each class a role:

SENSOR_SOURCE

LOGIC (Repository/UseCase/Manager/etc.)

VIEWMODEL

UI (Activity/Fragment/Composable)

GENERIC

Heuristics:

Extends ViewModel or annotated with @HiltViewModel → VIEWMODEL

Subclass of Activity, Fragment, ComponentActivity → UI

@Composable functions → UI-related nodes

Uses sensor APIs directly → candidate SENSOR_SOURCE

Reads/writes persistent storage (preferences, db) and called by other logic → candidate LOGIC

Else: GENERIC

These roles are best-effort, not required for correctness.

Step 7 – Flow Extraction

A flow is a path starting from a SENSOR_SOURCE node and eventually reaching:

UI nodes

Data sinks (e.g., logs, files, network)

Example expected flow in SenseMap:

AccelerometerListener
  → SensorRepository
    → MotionProcessor / ReflectionRepository (logic)
      → SenseViewModel
        → SenseScreen (Composable)


Each step corresponds to one or more CALLS and/or WRITES_STATE edges.

Current implementation (only MainActivity → repos) is not a flow; it’s a shallow reference graph.

10. Graph Model (Detailed)
10.1 Node
Node {
  id: UUID
  name: String          // e.g., "SensorRepository", "MainActivity"
  fqName: String        // fully qualified name
  kind: NodeKind        // CLASS, METHOD, INTERFACE, OBJECT
  role: NodeRole        // SENSOR_SOURCE, LOGIC, VIEWMODEL, UI, GENERIC
  filePath: String
  methods: List<MethodRef>
  sensors: List<SensorType>    // if any
  metadata: NodeMetadata
}


NodeMetadata example:

NodeMetadata {
  lifecycleRelated: Boolean
  threadHint: ThreadHint   // MAIN, BACKGROUND, UNKNOWN
  stateExposure: List<StateExposure> // LiveData, Flow, etc.
}

10.2 Edge
Edge {
  id: UUID
  from: NodeId
  to: NodeId
  type: EdgeType   // CALLS, USES_SENSOR_DATA, WRITES_STATE, READS_STATE, LIFECYCLE_LINK
  label: String?   // optional: method name, state name
  sourceLocation: SourceLocation?
}


EdgeType must not be left empty; this is exactly what’s missing now.

10.3 Flow
Flow {
  id: UUID
  sensorType: SensorType
  path: List<NodeId>
  confidence: Float  // 1.0 when fully static, <1.0 when partial
}

11. Handling Messy/Legacy Codebases

No assumption of clean layers

Even if everything is in one class, we still:

Identify sensor usage

Build method-level call graph

If reflection or DI hides dependencies:

Mark edges as partial/uncertain

Expose this in UI and AI answers

AI should say:

“Some relationships cannot be fully resolved due to reflection or indirect wiring.”

12. Issue Detection
12.1 Rule Engine

Runs on:

Nodes

Edges

Flows

Code snippets

12.2 Issue Structure
Issue {
  id: UUID
  type: IssueType
  severity: LOW | MEDIUM | HIGH
  description: String
  nodeIds: List<NodeId>
  locations: List<SourceLocation>
  recommendation: String
}

12.3 MVP Rules

Listener not unregistered

Heavy work in sensor callbacks (on main thread)

Excessive sampling (SENSOR_DELAY_FASTEST without filtering)

Duplicate listener registrations

Missing permission checks around sensitive APIs

13. AI Integration
13.1 Modes

OFFLINE: AI disabled or minimal local heuristics

ONLINE: Full dev-like explanations using external LLM

13.2 Providers

Google/Gemini via OAuth-based connection

OpenAI via user-provided API key

13.3 Q&A Behavior

Context:

Current file / node / flow selection

Related nodes/edges/issues

AI prompt includes:

Only relevant graph facts

Selected code snippets

User question

Guardrails:

Only use provided info

Admit when unknown

Be concise and technical

Answers:

Short explanation

Links:

Nodes (graph highlight)

Source locations (code view)

14. Local Storage & Cache

Per-project cache:

Graph

Flows

Issues

Version tag or hash

Supports incremental re-analysis

API tokens stored encrypted, separate from project

15. Git & .gitignore
15.1 Template

In GitHub:

Choose .gitignore → Gradle

15.2 Manual Additions
# IntelliJ / JetBrains
.idea/
*.iml
out/

# Logs
*.log

# Local settings
*.local
*.cache

# API keys / secrets
.env
*.env
*.keys
apikeys.json
tokens.json

# Optional Compose Desktop caches
compose-cache/


Don’t commit:

Build outputs

IDE settings

Secrets

Logs

16. Performance & Limits

Small projects: full analysis < 3s

Medium: < 10s

Large: allow partial views, grouping, incremental re-analysis

Graph UI:

Collapse/expand large structures

Lazy-loading node details

17. Phased Plan (with Current Gap Highlighted)
Phase 1 – Skeleton (what you mostly have now)

UI shell

Folder tree

Basic engine: file-level scanning, node discovery

Graph panel with nodes

Edges based on imports/references only (what you’re seeing)

No edge types, no flows

➡️ Current gap: Edges are shallow and unlabeled — needs upgrading.

Phase 2 – Real Static Engine (next critical step)

Implement method-level call graph extraction

Implement sensor entry detection at method level

Implement typed edges (CALLS, USES_SENSOR_DATA, etc.)

Implement flow extraction from SENSOR_SOURCE → UI

Goal:
SenseMap-like repo should show real flows, not just “MainActivity → everything.”

Phase 3 – Issue Engine & Better Graph

Run rules over full graph

Show issues in details panel

Visual emphasis for risky nodes/edges

Phase 4 – AI Integration

Connections menu

Providers

Contextual prompt assembly

Dev-like Q&A

Phase 5 – UX & Power Features

Filters

Export flows

User annotations

Graph grouping