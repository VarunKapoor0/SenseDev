Commercial AI Capability Requirements for SenseDev

Version 1.1 ‚Äî Requirements, Code Access, Gemini 2.x Support & Model Selection UI

(This document defines the minimum capabilities required from commercial AI models AND the SenseDev UI needed to fully access those capabilities.)

1. Purpose of This Specification

SenseDev requires the ability to explain Android/Kotlin codebases using commercial AI models (Gemini, OpenAI, etc.). This requires:

Code snippet injection

Correct prompt assembly

Proper response parsing

Gemini 2.x compatibility

A visible, intuitive model selection UI

This document outlines all requirements.

2. Required AI Capabilities

Commercial AI must support:

Reading arbitrary source code text

Reasoning across the codebase

Combining graph + code context

Code-level explanations

Non-hallucinatory, grounded answers

Handling Kotlin, Compose, Java idioms

Multi-file reasoning

Flow + lifecycle reasoning

3. Codebase Access Requirements

SenseDev must extract and send only the minimal necessary code context, in four tiers:

Tier 1: Selected code (method/class)

Tier 2: Graph-adjacent code

Tier 3: Flow-based code paths

Tier 4: Full-file (opt-in only)

Snippet size must remain within token limits.

4. Prompt Construction Requirements

Prompts must include:

User question

Graph context

Code snippet(s)

Constraints

Expected answer format

And MUST use Gemini‚Äôs multi-part contents[].parts[] structure.

5. Gemini 2.x Requirements

SenseDev must support:

gemini-2.5-flash

gemini-2.0-pro

gemini-2.0-flash

And must:

Query v1beta/models

Parse new response schemas

Use updated prompt format

Support multi-part messages

Handle larger token windows

6. Security & Privacy Requirements

Never send whole project automatically

Never send secrets or configs

Code access requires explicit user permission

Show preview of code being sent

Provide ‚ÄúSnippet-only‚Äù mode by default

7. Failure Modes & Expected Behavior

Empty LLM response ‚Üí ‚ÄúNo response from AI‚Äù

Parsing failure ‚Üí ‚ÄúUnsupported LLM response format‚Äù

Token overflow ‚Üí Auto-truncate

Missing code ‚Üí ‚ÄúCannot answer without source code‚Äù

üìò 8. Model Selection UI Specification (Integrated)

(This is the new section requested by you.)

This defines the UI that allows the user to select Gemini/OpenAI models, manage API keys, test connectivity, and configure code-access levels.

8.1 Location in the Application

The Model Selection UI will appear under the top menu:

Connections ‚Üí AI Providers


When clicked, it opens a right-side or center popup panel titled:

‚ÄúAI Provider Settings‚Äù

8.2 UI Structure

The UI contains the following sections:

+------------------------------------------------------+
|               AI Provider Settings                   |
+------------------------------------------------------+
| Provider: [ OpenAI ‚ñº ]   OR    [ Gemini ‚ñº ]         |
|                                                      |
| API Key: [ ************ ]  (Show) (Paste new)        |
| Status:  ‚óè Connected / ‚óã Not Connected               |
|                                                      |
| Available Models:                                    |
|   (Dropdown populated dynamically from /models)      |
|   [ gemini-2.5-flash ‚ñº ]                             |
|                                                      |
| Test Model:   [ Run Test ]                           |
| Result:       ‚ÄúModel responded successfully.‚Äù        |
|                                                      |
| Code Access Level:                                   |
|   ( ) Snippet-only (safe default)                    |
|   ( ) Flow-level context (multi-file)                |
|   ( ) Full file access (explicit opt-in)             |
|                                                      |
| Maximum Code Tokens Allowed: [ 2000 ]                |
|                                                      |
| Save Settings    Cancel                              |
+------------------------------------------------------+

8.3 Detailed Functional Requirements
8.3.1 Provider Selection Dropdown

Values:

‚ÄúGemini (AI Studio REST)‚Äù

‚ÄúOpenAI API‚Äù

‚ÄúAnthropic (future)‚Äù

‚ÄúLocal Model (future)‚Äù

Selecting a provider updates:

Required fields

Model dropdown

API key validator

Prompt formats

8.3.2 API Key Input

Features:

Hidden by default

‚ÄúPaste API Key‚Äù button

‚ÄúShow Key‚Äù toggle

Stored encrypted locally

Validation steps:

Check non-empty

Call /models?key= for Gemini

If successful ‚Üí ‚ÄúConnected‚Äù

Else ‚Üí surface specific error

8.3.3 Dynamic Model Dropdown
How it works:

When a provider is selected and key is present:

SenseDev calls:

Gemini:
GET https://generativelanguage.googleapis.com/v1beta/models?key=...

OpenAI:
GET https://api.openai.com/v1/models

Parse list of models

Filter to supported ones (by schema compatibility)

Populate dropdown

Example Gemini 2.x list:

gemini-2.5-flash

gemini-2.0-pro

gemini-2.0-flash

learnlm-1.1-pro

Models NOT supported (wrong schema) must be hidden or marked ‚Äúunsupported‚Äù.

8.3.4 Test Model Button

Runs a test query:

contents:[ { parts:[ {text:"Hello from SenseDev"} ] } ]


If successful:

Show last response (first 150 chars)

Mark provider as ‚ÄúConnected‚Äù

If not:

Show error reason

Suggest possible fixes (e.g., ‚Äú403: invalid key‚Äù, ‚Äú404: restricted model‚Äù)

8.3.5 Code Access Level Selector

Three options:

(A) Snippet-only (default)

Only selected method/class

Max 300‚Äì600 tokens

Minimal privacy impact

(B) Flow-level

Include graph-adjacent nodes

Include methods from flow nodes

Max ~1500‚Äì3000 tokens

(C) Full File Access (Explicit!)

Checkbox must appear:

[ ] I understand that file contents will be sent to the AI.


Users must opt-in

Whole files truncated to safe length

8.3.6 Token Limit Entry

Field:

Max code tokens: [ 2000 ]


Defines how much code SenseDev will include when constructing prompts.

This prevents:

Exceeding LLM token budget

Sudden huge costs

8.3.7 Save / Cancel Behavior

‚ÄúSave‚Äù writes settings to encrypted local config

‚ÄúCancel‚Äù restores previous values

8.4 UX Requirements
A) Error Messaging Must Be Plain and Actionable

Examples:

‚ÄúThis API key does not have access to Gemini models.‚Äù

‚ÄúYour account does not support gemini-2.5-flash. Try gemini-2.0-flash.‚Äù

‚ÄúModel returned unsupported response format.‚Äù

B) Never Block User on AI Setup

SenseDev must operate fully using Tier 1 local AI.

C) Provider UI Must Be Out of the Way

Only power users use it often.

8.5 Internal Technical Requirements

To support this UI:

SenseDev must implement:

A /models fetcher

A model capability registry

A Gemini 2.x response parser

Per-provider serialization logic

Config persistence

Automatic fallback if model errors

8.6 Security Requirements

Never send API keys to third parties

Keys stored AES-encrypted locally

No unencrypted key writing

Code snippets shown to users before sending

Full-file mode requires explicit consent

9. Summary of Requirements
Commercial AI must:

Parse Kotlin/Java/Compose

Reason over multi-file flows

Handle Gemini 2.x schema

Accept multi-part messages

Provide grounded, non-hallucinatory answers

SenseDev must:

Provide snippet-level and file-level code context

Assemble structured prompts

Parse new Gemini 2.x schemas

Support dynamic model selection

Provide a clear, safe UI for provider setup